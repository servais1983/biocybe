#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
BioCybe - Explications des d√©cisions d'IA

Ce module fournit des classes pour rendre les d√©cisions d'IA explicables
et interpr√©tables, en s'inspirant des principes de l'IA explicable (XAI).
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import shap
import lime
import lime.lime_tabular
from captum.attr import IntegratedGradients, DeepLift, GradientShap
from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve
import logging

logger = logging.getLogger(__name__)


class ExplainableDecision:
    """
    Classe de base pour rendre les d√©cisions d'IA explicables.
    Cette classe encapsule une d√©cision d'IA et fournit des m√©thodes
    pour l'expliquer.
    """

    def __init__(self, model, model_type, feature_names=None):
        """
        Initialiser un objet ExplainableDecision.

        Args:
            model: Le mod√®le d'IA √† expliquer
            model_type: Le type de mod√®le ('neural_network', 'tree', 'ensemble', etc.)
            feature_names: Liste des noms des fonctionnalit√©s du mod√®le
        """
        self.model = model
        self.model_type = model_type
        self.feature_names = feature_names
        self.explainers = {}
        self._initialize_explainers()

    def _initialize_explainers(self):
        """Initialise les explainers appropri√©s selon le type de mod√®le."""
        try:
            if 'neural_network' in self.model_type:
                # Explainers for neural networks
                if hasattr(self.model, 'predict'):
                    self.explainers['shap'] = shap.DeepExplainer(self.model, np.zeros((1, len(self.feature_names))))
                    
                    # Captum explainers (pour les mod√®les PyTorch)
                    if 'torch' in str(type(self.model)):
                        self.explainers['integrated_gradients'] = IntegratedGradients(self.model)
                        self.explainers['deep_lift'] = DeepLift(self.model)
                        self.explainers['gradient_shap'] = GradientShap(self.model)

            elif 'tree' in self.model_type or 'ensemble' in self.model_type:
                # TreeExplainer for tree-based models (Random Forest, XGBoost, etc.)
                self.explainers['shap'] = shap.TreeExplainer(self.model)

            else:
                # KernelExplainer as a fallback for all models
                self.explainers['shap'] = shap.KernelExplainer(self.model.predict, np.zeros((1, len(self.feature_names))))
                
            # LIME explainer for all models
            self.explainers['lime'] = lime.lime_tabular.LimeTabularExplainer(
                np.zeros((1, len(self.feature_names))),
                feature_names=self.feature_names,
                discretize_continuous=True
            )
            
            logger.info(f"Initialized explainers for {self.model_type} model: {list(self.explainers.keys())}")
        except Exception as e:
            logger.error(f"Error initializing explainers: {str(e)}")
            raise

    def explain_prediction(self, input_data, method='shap', num_features=10):
        """
        Explique une pr√©diction particuli√®re.
        
        Args:
            input_data: Les donn√©es d'entr√©e pour la pr√©diction √† expliquer
            method: La m√©thode d'explication √† utiliser ('shap', 'lime', etc.)
            num_features: Nombre de caract√©ristiques √† inclure dans l'explication
            
        Returns:
            Un dictionnaire contenant les informations d'explication
        """
        explanation = {'method': method, 'features': []}
        
        try:
            if method == 'shap' and 'shap' in self.explainers:
                # Calcul des valeurs SHAP
                shap_values = self.explainers['shap'].shap_values(input_data)
                
                # Format des donn√©es d'explication
                if isinstance(shap_values, list):
                    # Classification multi-classe
                    shap_values = shap_values[0]  # On prend la premi√®re classe pour simplifier
                
                # Cr√©ation d'un DataFrame avec les valeurs d'importance
                importance_df = pd.DataFrame({
                    'feature': self.feature_names,
                    'importance': np.abs(shap_values).mean(axis=0) if len(shap_values.shape) > 1 else np.abs(shap_values),
                    'value': input_data[0] if input_data.ndim > 1 else input_data
                })
                
                # Tri par importance d√©croissante
                importance_df = importance_df.sort_values('importance', ascending=False).head(num_features)
                
                # Ajout √† l'explication
                explanation['features'] = importance_df.to_dict(orient='records')
                
            elif method == 'lime' and 'lime' in self.explainers:
                # Explication LIME
                lime_exp = self.explainers['lime'].explain_instance(
                    input_data[0] if input_data.ndim > 1 else input_data,
                    self.model.predict_proba,
                    num_features=num_features
                )
                
                # Extraction des caract√©ristiques les plus importantes
                features = lime_exp.as_list()
                explanation['features'] = [{'feature': f[0], 'importance': f[1]} for f in features]
            
            else:
                raise ValueError(f"Unsupported explanation method: {method}")
            
            logger.info(f"Generated explanation using {method} with {len(explanation['features'])} features")
            return explanation
            
        except Exception as e:
            logger.error(f"Error generating explanation: {str(e)}")
            explanation['error'] = str(e)
            return explanation

    def generate_explanation_text(self, explanation):
        """
        G√©n√®re une explication en langage naturel √† partir des donn√©es d'explication.
        
        Args:
            explanation: Dictionnaire d'explication g√©n√©r√© par explain_prediction
            
        Returns:
            str: Explication en langage naturel
        """
        try:
            if 'error' in explanation:
                return f"Impossible de g√©n√©rer une explication: {explanation['error']}"
                
            # Construction du texte d'explication
            text = []
            text.append(f"Explication de la d√©cision (m√©thode: {explanation['method']}):")
            text.append("")
            
            # Ajoute les caract√©ristiques les plus importantes
            text.append("Facteurs les plus importants dans cette d√©cision:")
            
            for i, feature in enumerate(explanation['features'][:5]):  # Top 5 features
                importance = abs(feature['importance'])
                feature_name = feature['feature']
                
                # Formatage de l'importance pour la lisibilit√©
                if importance < 0.01:
                    importance_str = f"{importance:.6f}"
                else:
                    importance_str = f"{importance:.4f}"
                
                # Direction de l'impact (positif ou n√©gatif)
                direction = "augment√©" if feature['importance'] > 0 else "diminu√©"
                
                text.append(f"  {i+1}. {feature_name}: a {direction} le risque (importance: {importance_str})")
            
            return "\n".join(text)
            
        except Exception as e:
            logger.error(f"Error generating explanation text: {str(e)}")
            return f"Erreur lors de la g√©n√©ration de l'explication: {str(e)}"


class DecisionVisualizer:
    """
    Classe pour visualiser les explications de d√©cisions.
    """
    
    def __init__(self, dark_mode=False):
        """
        Initialise le visualiseur de d√©cisions.
        
        Args:
            dark_mode: Si True, utilise un th√®me sombre pour les visualisations
        """
        self.dark_mode = dark_mode
        self._set_plot_style()
    
    def _set_plot_style(self):
        """Configure le style des graphiques selon le mode."""
        if self.dark_mode:
            plt.style.use('dark_background')
        else:
            plt.style.use('default')
    
    def plot_feature_importance(self, explanation, title="Importance des caract√©ristiques", figsize=(10, 6)):
        """
        G√©n√®re un graphique d'importance des caract√©ristiques.
        
        Args:
            explanation: Dictionnaire d'explication
            title: Titre du graphique
            figsize: Taille de la figure (largeur, hauteur)
            
        Returns:
            matplotlib.figure.Figure: La figure g√©n√©r√©e
        """
        try:
            fig, ax = plt.subplots(figsize=figsize)
            
            # Extraction des donn√©es
            features = [f['feature'] for f in explanation['features']]
            importances = [f['importance'] for f in explanation['features']]
            
            # Cr√©ation du graphique √† barres horizontales
            ax.barh(range(len(features)), importances, color='#3498db')
            ax.set_yticks(range(len(features)))
            ax.set_yticklabels(features)
            ax.set_xlabel('Importance')
            ax.set_title(title)
            
            # Inversion pour avoir la plus importante en haut
            ax.invert_yaxis()
            
            plt.tight_layout()
            return fig
            
        except Exception as e:
            logger.error(f"Error plotting feature importance: {str(e)}")
            fig, ax = plt.subplots(figsize=(8, 2))
            ax.text(0.5, 0.5, f"Erreur lors de la cr√©ation du graphique: {str(e)}",
                    ha='center', va='center')
            return fig
    
    def plot_shap_summary(self, explainer, X_sample, max_display=20):
        """
        G√©n√®re un r√©sum√© SHAP pour un ensemble de donn√©es.
        
        Args:
            explainer: Un explainer SHAP initialis√©
            X_sample: √âchantillon de donn√©es pour l'explication
            max_display: Nombre maximum de caract√©ristiques √† afficher
            
        Returns:
            matplotlib.figure.Figure: La figure g√©n√©r√©e
        """
        try:
            # Calcul des valeurs SHAP
            shap_values = explainer.shap_values(X_sample)
            
            # Cr√©ation de la figure
            fig = plt.figure(figsize=(10, 8))
            
            # Cr√©ation du r√©sum√© SHAP
            if isinstance(shap_values, list):
                # Multi-classe - on prend la premi√®re classe
                shap.summary_plot(shap_values[0], X_sample, show=False, max_display=max_display)
            else:
                # Binaire ou r√©gression
                shap.summary_plot(shap_values, X_sample, show=False, max_display=max_display)
            
            plt.tight_layout()
            return fig
            
        except Exception as e:
            logger.error(f"Error plotting SHAP summary: {str(e)}")
            fig, ax = plt.subplots(figsize=(8, 2))
            ax.text(0.5, 0.5, f"Erreur lors de la cr√©ation du r√©sum√© SHAP: {str(e)}",
                    ha='center', va='center')
            return fig
    
    def plot_decision_tree(self, tree_model, feature_names, class_names=None, max_depth=3, figsize=(15, 10)):
        """
        Visualise un arbre de d√©cision pour une meilleure explicabilit√©.
        
        Args:
            tree_model: Un mod√®le d'arbre de d√©cision
            feature_names: Noms des caract√©ristiques
            class_names: Noms des classes (pour les mod√®les de classification)
            max_depth: Profondeur maximale de l'arbre √† visualiser
            figsize: Taille de la figure
            
        Returns:
            matplotlib.figure.Figure: La figure g√©n√©r√©e
        """
        try:
            from sklearn.tree import plot_tree
            
            fig, ax = plt.subplots(figsize=figsize)
            plot_tree(tree_model, 
                      feature_names=feature_names, 
                      class_names=class_names,
                      filled=True, 
                      rounded=True, 
                      max_depth=max_depth,
                      ax=ax)
            
            plt.tight_layout()
            return fig
            
        except Exception as e:
            logger.error(f"Error plotting decision tree: {str(e)}")
            fig, ax = plt.subplots(figsize=(8, 2))
            ax.text(0.5, 0.5, f"Erreur lors de la visualisation de l'arbre: {str(e)}",
                    ha='center', va='center')
            return fig


class ThreatExplainer:
    """
    Classe sp√©cialis√©e pour expliquer les d√©tections de menaces.
    """
    
    def __init__(self, detection_model, feature_names, threshold=0.5):
        """
        Initialise l'expliqueur de menaces.
        
        Args:
            detection_model: Le mod√®le de d√©tection de menaces
            feature_names: Noms des caract√©ristiques utilis√©es par le mod√®le
            threshold: Seuil de d√©cision pour la classification des menaces
        """
        self.model = detection_model
        self.feature_names = feature_names
        self.threshold = threshold
        self.explainable_decision = ExplainableDecision(detection_model, 'neural_network', feature_names)
    
    def explain_threat_detection(self, input_data, explanation_method='shap'):
        """
        Explique pourquoi une d√©tection de menace a √©t√© d√©clench√©e.
        
        Args:
            input_data: Donn√©es d'entr√©e qui ont d√©clench√© la d√©tection
            explanation_method: M√©thode d'explication √† utiliser
            
        Returns:
            dict: Explication d√©taill√©e de la d√©tection
        """
        # Obtenir la pr√©diction
        prediction = self.model.predict(input_data)
        prediction_proba = self.model.predict_proba(input_data) if hasattr(self.model, 'predict_proba') else None
        
        # D√©terminer si c'est une menace
        is_threat = prediction[0] > self.threshold if hasattr(prediction, '__iter__') else prediction > self.threshold
        
        # Pr√©parer le r√©sultat
        threat_explanation = {
            'is_threat': bool(is_threat),
            'confidence': float(prediction_proba[0][1]) if prediction_proba is not None else float(prediction[0]),
            'threshold': self.threshold
        }
        
        # Obtenir l'explication d√©taill√©e
        explanation = self.explainable_decision.explain_prediction(input_data, method=explanation_method)
        threat_explanation['explanation'] = explanation
        
        # G√©n√©rer le texte d'explication
        threat_explanation['explanation_text'] = self.generate_threat_explanation_text(threat_explanation)
        
        return threat_explanation
    
    def generate_threat_explanation_text(self, threat_explanation):
        """
        G√©n√®re une explication en langage naturel pour une d√©tection de menace.
        
        Args:
            threat_explanation: Dictionnaire contenant l'explication de la menace
            
        Returns:
            str: Explication en langage naturel
        """
        is_threat = threat_explanation['is_threat']
        confidence = threat_explanation['confidence'] * 100  # Conversion en pourcentage
        
        # Construction du texte d'explication
        if is_threat:
            text = [
                f"üî¥ ALERTE: D√©tection d'une activit√© suspecte (confiance: {confidence:.1f}%)",
                "",
                "BioCybe a identifi√© une menace potentielle bas√©e sur les indicateurs suivants:"
            ]
        else:
            text = [
                f"üü¢ Information: Activit√© analys√©e et jug√©e s√ªre (confiance: {100-confidence:.1f}%)",
                "",
                "BioCybe a analys√© cette activit√© et l'a jug√©e b√©nigne pour les raisons suivantes:"
            ]
        
        # Ajouter les facteurs les plus importants
        for i, feature in enumerate(threat_explanation['explanation']['features'][:5]):
            feature_name = feature['feature']
            importance = feature['importance']
            direction = "augment√©" if importance > 0 else "diminu√©"
            
            text.append(f"  {i+1}. {feature_name}: a {direction} le niveau de risque")
        
        # Ajouter des conseils si c'est une menace
        if is_threat:
            text.append("")
            text.append("Actions recommand√©es:")
            text.append("  - Isoler le processus suspect")
            text.append("  - Examiner les journaux d'activit√©")
            text.append("  - V√©rifier les connexions r√©seau associ√©es")
        
        return "\n".join(text)
